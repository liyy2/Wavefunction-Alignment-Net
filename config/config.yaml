defaults:
- model: equiformerv2
- schedule: polynomial #'cosine', 'reduce_on_plateau','polynomial' 
# - data_module: default
- _self_


job_id: lsr_equiv2_sp07_sym_o4
seed: 1
log_dir: "./tmp/logs"
debug: false

wandb:
  open: True
  wandb_api_key: c77ec9804aaffba9a04e3b7b4dd389b53210ab39
  wandb_project: MADFT-NN-test
  wandb_group: ""
  wandb_notes: ""

#########
# trainer related config
model_backbone: Equiformerv2SO2
output_model: EquivariantScalar_viaTP
hami_model:
  name: HamiHeadSymmetry
  irreps_edge_embedding: null
  num_layer: 2
  max_radius_cutoff: 30  # hami head cut off, default full connected
  # irrep_in_node = representation_model.irreps_node_embedding,
  # order = model_backbone_cfg['order'],
  # pyscf_basis_name = basis
use_sparse_tp: true
num_epochs: 3000000 #number of epochs
max_steps: 100000 #Maximum number of gradient steps.
batch_size: 2 #batch size
inference_batch_size: null
dataloader_num_workers: 4
lr: 5e-4
multi_para_group: false
weight_decay: 0
enable_hami: true
enable_symmetry: false
enable_energy: false
enable_forces: false
enable_energy_hami_error: false # when model training, 1bs validation data will calculate this error due to time cost.
energy_weight: 0 #Weighting factor for energies in the loss function
forces_weight: 0 #Weighting factor for forces in the loss function
hami_weight: 1 #Weighting factor for hami in the loss function
energy_train_loss: 'mse'
forces_train_loss: 'mse'
hami_train_loss: 'maemse'
energy_val_loss: 'mae'
forces_val_loss: 'mae'
hami_val_loss: 'mae'
sparse_loss: false
sparse_loss_coeff: 1e-5
ngpus: 1
num_nodes: 1
precision: 32 # the precison for training, can use 16 for mixed precision
gradient_clip_val: 1.0
early_stopping_patience: 300000
val_check_interval: null #follow pytorch lightning
test_interval: 10 #Test interval, one test per n epochs (default: 10) # NOT used
save_interval: 5 #Save interval, one save per n epochs (default: 10)
############
# data realted config
data_name: pubchem
basis: "def2-tzvp"  #when predict hamitonian, the basis need to be set
dataset_path : "data_pool/pubchem"
dataset_size : -1 #the dataset size is used for debug. -1 is all data")
train_ratio: 0.9
val_ratio: 0.06
test_ratio: 0.002
cutoff_lower: 0.0 #Lower cutoff in model
cutoff_upper: 5.0 #Upper cutoff in model
used_cache: false
ema_decay: 1 # 1 means trun off ema and (0,1) is turning on.
unit: 1 # 627.503
############
# model related config
activation: "silu"
remove_init: true # fock - init if remove_init else fock
remove_atomref_energy: true # when true, energy = energy - eachtype_atom_count*atom_ref
num_sanity_val_steps: 0