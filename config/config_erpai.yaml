defaults:
- model: equiformerv2 #qhnet #equiformerv2
- schedule: polynomial #'cosine', 'reduce_on_plateau','polynomial' 
# - data_module: default
- _self_


job_id: new_qh9_equiv2_sp07_o2_f128_lr5e4_15w_bs128
seed: 1
# log_dir: "/mnt/data/projects/hami_accelerate/ckpt/logs"
log_dir: "./tmp/logs"
debug: false

wandb:
  open: False
  wandb_api_key: c77ec9804aaffba9a04e3b7b4dd389b53210ab39
  wandb_project: MADFT-NN
  wandb_group: "test"
  wandb_notes: ""

#########
# trainer related config
model_backbone: Equiformerv2SO2 #QHNet_backbone #Equiformerv2SO2
# model_backbone: QHNet_backbone
output_model: EquivariantScalar_viaTP
hami_model:
  # name: HamiHead
  name: HamiHead #HamiHeadSymmetry
  irreps_edge_embedding: null
  num_layer: 2
  max_radius_cutoff: 30  # hami head cut off, default full connected
  # irrep_in_node = representation_model.irreps_node_embedding,
  # order = model_backbone_cfg['order'],
  # pyscf_basis_name = basis
use_sparse_tp: true
num_epochs: 30 #number of epochs
max_steps: 150000 #Maximum number of gradient steps.
batch_size: 1 #batch size
inference_batch_size: null
dataloader_num_workers: 1
lr: 5e-4
multi_para_group: false
weight_decay: 0
enable_hami: true
enable_symmetry: false
enable_energy: false
enable_forces: false
enable_energy_hami_error: false # when model training, 1bs validation data will calculate this error due to time cost.
energy_weight: 0 #Weighting factor for energies in the loss function
forces_weight: 0 #Weighting factor for forces in the loss function
hami_weight: 1 #Weighting factor for hami in the loss function
energy_train_loss: 'mse'
forces_train_loss: 'mse'
hami_train_loss: 'maemse'
energy_val_loss: 'mae'
forces_val_loss: 'mae'
hami_val_loss: 'mae'
sparse_loss: false
sparse_loss_coeff: 1e-5
# orbital_energy_train_loss: 'mse'
ngpus: 1
num_nodes: 1
precision: 32 # the precison for training, can use 16 for mixed precision
gradient_clip_val: 1.0
early_stopping_patience: 300000
val_check_interval: null #follow pytorch lightning
test_interval: 10 #Test interval, one test per n epochs (default: 10) # NOT used
save_interval: 5 #Save interval, one save per n epochs (default: 10)
############
# data realted config
# data_name: pubchem
# basis: "def2-tzvp"  #when predict hamitonian, the basis need to be set
# dataset_path : "data_pool/pubchem"
data_name: pubchem
basis: "def2-tzvp"  #when predict hamitonian, the basis need to be set
dataset_path : "/home/weixinran/MADFT-NN/local_files/waters_tzvp_30"
dataset_size : -1 #the dataset size is used for debug. -1 is all data")
train_ratio: 1
val_ratio: 0.00
test_ratio: 0.00
cutoff_lower: 0.0 #Lower cutoff in model
cutoff_upper: 5.0 #Upper cutoff in model
used_cache: false
ema_decay: 1 # 1 means trun off ema and (0,1) is turning on.
unit: 1 # 627.503
############
# model related config
activation: "silu"
remove_init: true # fock - init if remove_init else fock
remove_atomref_energy: true # when true, energy = energy - eachtype_atom_count*atom_ref
num_sanity_val_steps: 0