order : 3
embedding_dimension: 96
bottle_hidden_size: 32
num_gnn_layers: 4
radius_embed_dim: 16
use_equi_norm: False
start_layer: 2
load_pretrain: ''

# para that only used by lsm-qhnet
short_cutoff_upper: 5
long_cutoff_upper: 15
num_scale_atom_layers: 4
num_long_range_layers: 2

use_pbc:                  False
regress_forces:           False
otf_graph:                False
max_neighbors:            20
max_radius:               12.0
max_num_elements:         90
sphere_channels:          128
attn_hidden_channels:     64              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96.
num_heads:                8
attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True.
attn_value_channels:      16
ffn_hidden_channels:      128
norm_type:                'layer_norm_sh' # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']
lmax_list:                [4]
mmax_list:                [2]
grid_resolution:          18              # [18, 16, 14, None] For `None`, simply comment this line.
num_sphere_samples:       128
edge_channels:              128
use_atom_edge_embedding:    True
share_atom_edge_embedding:  False         # If `True`, `use_atom_edge_embedding` must be `True` and the atom edge embedding will be shared across all blocks. 
distance_function:          'gaussian'
num_distance_basis:         512         # not used
attn_activation:          'silu'
use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention. 
use_attn_renorm:          True        # Attention re-normalization. Used for ablation study.
ffn_activation:           'silu'      # ['silu', 'swiglu']
use_gate_act:             False       # [True, False] Switch between gate activation and S2 activation
use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.
use_sep_s2_act:           True        # Separable S2 activation. Used for ablation study.
alpha_drop:               0.1         # [0.0, 0.1]
drop_path_rate:           0.1         # [0.0, 0.05]
proj_drop:                0.0
weight_init:              'uniform'    # ['uniform', 'normal']