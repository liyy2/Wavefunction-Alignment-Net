defaults:
- model: qh9net
- schedule: null #'cosine', 'reduce_on_plateau','polynomiallr'  
# - data_module: default
- _self_


job_id: null
seed: 1
log_dir: "./tmp/logs"


wandb:
  open: True
  wandb_api_key: c1a2a42a876478b8af32e6ca37054a3692af047f
  wandb_project: MADFT-NN
  wandb_group: "zaishuo_HEF"
  wandb_notes: ""

#########
# trainer related config
model_backbone: QHNet_backbone_half
output_model: EquivariantScalar_viaTP
num_epochs: 3000000 #number of epochs
max_steps: 200000 #Maximum number of gradient steps.
batch_size: 256 #batch size
inference_batch_size: null
dataloader_num_workers: 4
lr: 5e-4
weight_decay: 0
enable_hami: false
enable_energy: false
enable_forces: false
energy_weight: 0 #Weighting factor for energies in the loss function
forces_weight: 0 #Weighting factor for forces in the loss function
hami_weight: 0 #Weighting factor for hami in the loss function
energy_train_loss: 'mse'
forces_train_loss: 'mse'
hami_train_loss: 'maemse'
energy_val_loss: 'mae'
forces_val_loss: 'mae'
hami_val_loss: 'mae'
ngpus: 4
num_nodes: 1
precision: 32 # the precison for training, can use 16 for mixed precision
gradient_clip_val: 5.0
early_stopping_patience: 300000
val_check_interval: null #follow pytorch lightning
test_interval: 10 #Test interval, one test per n epochs (default: 10)
save_interval: 10 #Save interval, one save per n epochs (default: 10)
############
# data realted config
basis: "def2-svp"  #when predict hamitonian, the basis need to be set
dataset_path : null
dataset_size : -1 #the dataset size is used for debug. -1 is all data")
train_ratio: null
val_ratio: null
test_ratio: null
cutoff_lower: 0.0 #Lower cutoff in model
cutoff_upper: 5.0 #Upper cutoff in model
used_cache: false
ema_decay: 1 # 1 means trun off ema and (0,1) is turning on.
############
# model related config
activation: "silu"