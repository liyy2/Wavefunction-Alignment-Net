# For field details: https://amulet-docs.azurewebsites.net/config_file.html
description: MADFT-NN

target:
  service: aml
  name: es-nc96-eastus


# environment:
#   image: huang6385/feynman:madft-nn-py310-nvcc118
environment:
  registry: msrmoldyn.azurecr.io
  image: feynman:madft-nn-py311torch210-py310torch120
  username: msrmoldyn
  # setup:
  #   - pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pytorch-lightning==2.1.2 torch_geometric
  #   - pip install torchvision==0.13.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pyg_lib==0.3.0+pt112cu113 torch_scatter==2.1.0+pt112cu113 torch_sparse==0.6.15+pt112cu113 torch_cluster==1.6.0+pt112cu113 torch_spline_conv==1.2.1+pt112cu113 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html


code:
    local_dir: ../
    ignore:
    - "checkpoints" # In principle, ignore all files,
    - "logs" # and those in projects/livdft,
    - "wandb" # and those in projects/livdft,
    - "experiments" # and those in projects/livdft,
    - "data"
    - "local_files"
storage:
    storageA:
      storage_account_name: qcacceleration
      container_name: yl-lsrm
      mount_dir: /mnt/data
data:
    local_dir: ../data/ ###NOTICE: only relative path can be used here
    remote_dir: dataset/
    storage_id: storageA
    


# SKU usage: G1 (single GPU), G4 (quad GPU), G4-V100 (1 machine, 4 V100 gpus), etc...
jobs:
  - name: A100-MADFT-NN-Zaishuo-0
    sku: 80G4-A100
    sla_tier: premium
    tags: [ProjectID:PRJ-0192-A37]
    command:
    - ls -hl
    - df -hl
    - pwd
    - eval "$$(conda shell.bash hook)"
    - conda activate madft_nn_torch210
    - export PYTHONPATH=$$PYTHONPATH:.
    - cp $$AMLT_DATA_DIR/QH9_new.db /dev/shm/
    - python pipelines/train.py --config-name=config.yaml job_id=qhnet_symmetry wandb.open=True wandb.wandb_group="Lin" wandb.wandb_api_key=21dcfbbb833c4ab9c4ccb098bd70e3298bddb800 \
    - dataset_path="$$AMLT_DATA_DIR/QH9_new.db" model_backbone=QHNet_backbone_symmetry ngpus=4 lr=0.0005 enable_hami=True \
    - hami_weight=1  batch_size=32 schedule=polynomial schedule.lr_warmup_steps=1000 \
    - max_steps=300000 used_cache=True \
    - train_ratio=0.9 val_ratio=0.06 test_ratio=0.04  gradient_clip_val=5.0 dataset_size=100000 model.use_equi_norm=True remove_init=False
    submit_args:
      env:
        {MKL_THREADING_LAYER: GNU}
      container_args:
        shm_size: 512g
    # priority: high
    # preemptible: false
  # - python scripts/train.py --conf examples/ET-PUBCHEM2Equi.yaml \
  #   - --wandb=True --wandb-group="Lin" --job-id="lr2e-4" --layernorm-on-vec whitened \
  #   - --dataset-path="$$AMLT_DATA_DIR/QH9.db" --ngpus=8 --lr=0.0002
 
