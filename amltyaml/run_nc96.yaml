# For field details: https://amulet-docs.azurewebsites.net/config_file.html
description: MADFT-NN

target:
  service: aml
  name: es-madft-nc96-eastus2


# environment:
#   image: huang6385/feynman:madft-nn-py310-nvcc118
environment:
  registry: msrmoldyn.azurecr.io
  image: feynman:madft-nn-py311torch210-py310torch120
  username: msrmoldyn
  # setup:
  #   - pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pytorch-lightning==2.1.2 torch_geometric
  #   - pip install torchvision==0.13.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pyg_lib==0.3.0+pt112cu113 torch_scatter==2.1.0+pt112cu113 torch_sparse==0.6.15+pt112cu113 torch_cluster==1.6.0+pt112cu113 torch_spline_conv==1.2.1+pt112cu113 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html


code:
    local_dir: ../
    ignore:
    - "checkpoints" # In principle, ignore all files,
    - "logs" # and those in projects/livdft,
    - "wandb" # and those in projects/livdft,
    - "experiments" # and those in projects/livdft,
    - "tmp"
    - "local_files"
storage:
    storageA:
      storage_account_name: ai4science0eastus
      container_name: madft-nn
      mount_dir: /mnt/data
data:
    local_dir: ../data/ ###NOTICE: only relative path can be used here
    remote_dir: dataset/
    storage_id: storageA
    

# priority: high
# preemptible: false
# SKU usage: G1 (single GPU), G4 (quad GPU), G4-V100 (1 machine, 4 V100 gpus), etc...
jobs:
  - name: A100-MADFT-NN-Xinran-test
    sku: 80G4-A100
    sla_tier: premium
    tags: [ProjectID:PRJ-0192-A37]
    identity: managed
    command:
    - ls -hl
    - df -hl
    - pwd
    - eval "$$(conda shell.bash hook)"
    - conda activate madft_nn_torch210
    - export PYTHONPATH=$PYTHONPATH:./src
    - mkdir /tmp/dataset
    - cp -r $$AMLT_DATA_DIR/pubchem_20230831_processed/* /tmp/dataset
    - python pipelines/train.py --config-name=config.yaml \
    - ngpus=4 lr=1e-3 \
    - wandb.open=True wandb.wandb_group="Zaishuo-pubchem" \
    - wandb.wandb_api_key=231fdd2c5f23b319b6fe54997e3454ec24684cf5 \
    - log_dir="$$AMLT_OUTPUT_DIR" data_name="pubchem" dataset_path="/tmp/dataset/data.0000.lmdb" \
    - basis="def2-tzvp" model.embedding_dimension=64 model.max_radius=15 \
    - schedule.lr_warmup_steps=1000 max_steps=300000 used_cache=False \
    - train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 \
    - output_model=EquivariantScalar_viaTP \
    - job_id=noninit_1e-3_only_diag_mean \
    - model_backbone=Equiformerv2SO2 \
    - model=equiformerv2 \
    - enable_hami=True enable_symmetry=True enable_energy=False enable_forces=False \
    - hami_weight=0.1 energy_weight=0.0 forces_weight=0 \
    - batch_size=16 hami_train_loss=zero hami_model.name=HamiHeadSymmetry_uuw \
    - enable_hami_orbital_energy=True orbital_mat_gt=True orbital_energy_weight=1
    # - ls -hl
    # - df -hl
    # - pwd
    # - eval "$$(conda shell.bash hook)"
    # - conda activate madft_nn_torch210
    # - export PYTHONPATH=$PYTHONPATH:./src
    # - mkdir /tmp/dataset
    # - cp -r $$AMLT_DATA_DIR/pubchem_20230831_processed/* /tmp/dataset
    # - python pipelines/train.py --config-name=config.yaml \
    # - ngpus=4 lr=0.001 \
    # - wandb.open=True wandb.wandb_group="Zaishuo-pubchem" \
    # - wandb.wandb_api_key=231fdd2c5f23b319b6fe54997e3454ec24684cf5 \
    # - log_dir="$$AMLT_OUTPUT_DIR" data_name="pubchem" dataset_path="/tmp/dataset" \
    # - basis="def2-tzvp" model.embedding_dimension=64 model.max_radius=5 \
    # - schedule.lr_warmup_steps=1000 max_steps=300000 used_cache=True \
    # - train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 \
    # - output_model=EquivariantScalar_viaTP \
    # - job_id=equiform_hef_sym \
    # - model_backbone=equiformer \
    # - model=equiformer \
    # - enable_hami=True enable_symmetry=True enable_energy=True enable_forces=True \
    # - hami_weight=2.39 energy_weight=0.01 forces_weight=0.6 \
    # - batch_size=8 schedule=polynomial
    # dataset_path="$$AMLT_DATA_DIR/pubchem_20230831_processed/data.0000.lmdb" \
    # - ls /tmp/dataset
    # - df -hl
    # - python pipelines/train.py --config-name=config.yaml \
    # - ngpus=4 lr=0.001 \
    # - wandb.open=True wandb.wandb_group="Zaishuo-pubchem" \
    # - wandb.wandb_api_key=231fdd2c5f23b319b6fe54997e3454ec24684cf5 \
    # - log_dir="$$AMLT_OUTPUT_DIR" data_name="pubchem" dataset_path="$$AMLT_DATA_DIR/pubchem_20230831_processed/data.0000.lmdb" \
    # - basis="def2-tzvp" model.embedding_dimension=64 model.max_radius=15 \
    # - schedule.lr_warmup_steps=1000 max_steps=300000 used_cache=True \
    # - train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 \
    # - output_model=EquivariantScalar_viaTP \
    # - job_id=ef_equi_small_norm \
    # - model_backbone=equiformer \
    # - model=equiformer \
    # - enable_hami=False enable_symmetry=False enable_energy=True enable_forces=True \
    # - hami_weight=0 energy_weight=0.23 forces_weight=2.77 \
    # - batch_size=8 schedule=polynomial #  model.use_equi_norm=True
    # - enable_hami=False enable_symmetry=False enable_energy=True enable_forces=True \
    # - hami_weight=0 energy_weight=0.23 forces_weight=2.77 \
    # - batch_size=8 schedule=polynomial
    # HEF
    # - python pipelines/train.py --config-name=config.yaml \
    # - ngpus=4 lr=0.001 \
    # - wandb.open=True wandb.wandb_group="Zaishuo-pubchem" \
    # - wandb.wandb_api_key=231fdd2c5f23b319b6fe54997e3454ec24684cf5 \
    # - log_dir="$$AMLT_OUTPUT_DIR" data_name="pubchem" dataset_path="/tmp/dataset" \
    # - basis="def2-tzvp" model.embedding_dimension=64 model.max_radius=5 \
    # - schedule.lr_warmup_steps=1000 max_steps=300000 used_cache=True \
    # - train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 \
    # - output_model=EquivariantScalar_viaTP \
    # - job_id=HEF_lsr_S \
    # - model_backbone=LSR_QHNet_backbone \
    # - model=qhnet \
    # - enable_hami=True enable_symmetry=True enable_energy=True enable_forces=True \
    # - hami_weight=2.39 energy_weight=0.01 forces_weight=0.6 \
    # - batch_size=8 schedule=polynomial #  model.use_equi_norm=True
    # - python pipelines/train.py --config-name=config.yaml \
    # - wandb.open=True wandb.wandb_group="equi3order" job_id='EF' wandb.wandb_api_key=c1a2a42a876478b8af32e6ca37054a3692af047f \
    # - data_name="pubchem" dataset_path="$$AMLT_DATA_DIR/pubchem_20230831_processed/data.0000.lmdb" \
    # - basis="def2-tzvp"  ngpus=4 lr=0.001 \
    # - enable_hami=False enable_energy=True enable_forces=True \
    # - hami_weight=0 energy_weight=0.23 forces_weight=2.77 \
    # - batch_size=8 schedule=polynomial schedule.lr_warmup_steps=1000 max_steps=300000 \
    # - used_cache=True train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 gradient_clip_val=5.0 \
    # - output_model=EquivariantScalar_viaTP_Order0 model_backbone=equiformer model=equiformer
    # HEF
    # - python pipelines/train.py --config-name=config.yaml \
    # - wandb.open=True wandb.wandb_group="equi3order" job_id='HEF' wandb.wandb_api_key=c1a2a42a876478b8af32e6ca37054a3692af047f \
    # - data_name="pubchem" dataset_path="$$AMLT_DATA_DIR/pubchem_20230831_processed/data.0000.lmdb" \
    # - basis="def2-tzvp"  ngpus=4 lr=0.001 \
    # - enable_hami=True enable_energy=True enable_forces=True hami_weight=2.39 energy_weight=0.01 forces_weight=0.6 \
    # - batch_size=8 schedule=polynomial schedule.lr_warmup_steps=1000 max_steps=300000 \
    # - used_cache=True train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 gradient_clip_val=5.0 \
    # - output_model=EquivariantScalar_viaTP_Order0 model_backbone=equiformer model=equiformer
    # ONlY Hami
    # - python pipelines/train.py --config-name=config.yaml \
    # - wandb.open=True wandb.wandb_group="equi3order" job_id='H' wandb.wandb_api_key=c1a2a42a876478b8af32e6ca37054a3692af047f \
    # - data_name="pubchem" dataset_path="$$AMLT_DATA_DIR/pubchem_20230831_processed/data.0000.lmdb" \
    # - basis="def2-tzvp"  ngpus=4 lr=0.001 \
    # - enable_hami=True enable_energy=False enable_forces=False hami_weight=1 energy_weight=0 forces_weight=0 \
    # - batch_size=8 schedule=polynomial schedule.lr_warmup_steps=1000 max_steps=300000 \
    # - used_cache=True train_ratio=0.9 val_ratio=0.06 test_ratio=0.04 gradient_clip_val=5.0 \
    # - output_model=EquivariantScalar_viaTP_Order0 model_backbone=equiformer model=equiformer
    submit_args:
      env:
        {MKL_THREADING_LAYER: GNU}
      container_args:
        shm_size: 512g
