# For field details: https://amulet-docs.azurewebsites.net/config_file.html
description: MADFT-NN

target:
  service: aml
  name: es-nc96-eastus

# environment:
#   image: huang6385/feynman:madft-nn-py310-nvcc118
environment:
  registry: msrmoldyn.azurecr.io
  image: feynman:madft-nn-py311torch210-py310torch120
  username: msrmoldyn
  # setup:
  #   - pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pytorch-lightning==2.1.2 torch_geometric
  #   - pip install torchvision==0.13.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
  #   - pip install pyg_lib==0.3.0+pt112cu113 torch_scatter==2.1.0+pt112cu113 torch_sparse==0.6.15+pt112cu113 torch_cluster==1.6.0+pt112cu113 torch_spline_conv==1.2.1+pt112cu113 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html


code:
    local_dir: ../
    ignore:
    - "local_files"
    - "checkpoints" # In principle, ignore all files,
    - "logs" # and those in projects/livdft,
    - "wandb" # and those in projects/livdft,
    - "experiments" # and those in projects/livdft,
    - "tmp"
storage:
    storageA:
      storage_account_name: ai4science0eastus
      container_name: madft-nn
      mount_dir: /mnt/data
data:
    local_dir: ../data/ ###NOTICE: only relative path can be used here
    remote_dir: dataset/
    storage_id: storageA
    

# priority: high
# preemptible: false
# SKU usage: G1 (single GPU), G4 (quad GPU), G4-V100 (1 machine, 4 V100 gpus), etc...
jobs:
  - name: A100-MADFT-NN-Zaishuo
    sku: 80G4-A100
    sla_tier: premium
    tags: [ProjectID:PRJ-0192-A37]
    command:
    - ls -hl
    - df -hl
    - pwd
    - eval "$$(conda shell.bash hook)"
    - conda activate madft_nn_torch210
    - export PYTHONPATH=$$PYTHONPATH:./src
    # - cp $$AMLT_DATA_DIR/phisnet/malondialdehyde.db /dev/shm/
    # - ls -lh /dev/shm/
    # - python pipelines/train.py --wandb=True --dataset-path="/dev/shm/malondialdehyde.db" \
    # - --ngpus=4 --lr=0.0005 --enable-hami=True --hami-weight=1  --batch-size=256 --lr-warmup-steps=1000 \
    # - --lr-schedule="polynomiallr" --max-steps=200000 --used-cache=True --train-ratio=25000 --val-ratio=500 \
    # - --gradient-clip-val=5.0 --dataset-size=26978
    # - cp $$AMLT_DATA_DIR/phisnet/ethanol.db /dev/shm/
    # - ls -lh /dev/shm/
    # - python pipelines/train.py --wandb=True --dataset-path="/dev/shm/ethanol.db" \
    # - --ngpus=4 --lr=0.0005 --enable-hami=True --hami-weight=1  --batch-size=256 --lr-warmup-steps=1000 \
    # - --lr-schedule="polynomiallr" --max-steps=200000 --used-cache=True --train-ratio=25000 --val-ratio=500 \
    # - --gradient-clip-val=5.0 --dataset-size=30000
    - cp $$AMLT_DATA_DIR/phisnet/water.db /dev/shm/
    - ls -lh /dev/shm/
    - python pipelines/train.py --config-name=config.yaml \
    - wandb.open=True wandb.wandb_group="HEF_weight" job_id="water" \
    - dataset_path="/dev/shm/water.db" ngpus=4 lr=0.0005 enable_hami=True enable_energy=True \
    - enable_forces=True hami_weight=1.42 energy_weight=0.16 forces_weight=1.42 batch_size=32 schedule=polynomial \
    - schedule.lr_warmup_steps=1000 max_steps=200000 used_cache=True \
    - train_ratio=500 val_ratio=500 gradient_clip_val=5.0 dataset_size=4999
    # - python pipelines/train.py --config-name=config_split.yaml \
    # - wandb.open=True wandb.wandb_group="HEF_split_diff_weight" job_id="water" \
    # - dataset_path="/dev/shm/water.db" ngpus=4 lr=0.0005 enable_hami=True enable_energy=True \
    # - enable_forces=True hami_weight=1.42 energy_weight=0.16 forces_weight=1.42 batch_size=32 schedule=polynomial \
    # - schedule.lr_warmup_steps=1000 max_steps=200000 used_cache=True \
    # - train_ratio=500 val_ratio=500 gradient_clip_val=5.0 dataset_size=4999
    # 
    # - ls -lh /dev/shm/
    # - python pipelines/train.py --config-name=config.yaml log_dir="$$AMLT_OUTPUT_DIR" \
    # - wandb.open=True wandb.wandb_group="H" job_id="water-bz256" \
    # - dataset_path="/dev/shm/water.db" ngpus=4 lr=0.0005 enable_hami=True \
    # - hami_weight=1  batch_size=256 schedule=polynomial schedule.lr_warmup_steps=1000 \
    # - max_steps=200000 used_cache=True \
    # - train_ratio=500 val_ratio=500 gradient_clip_val=5.0 dataset_size=4999
    # - python pipelines/train.py --wandb=True --dataset-path="/dev/shm/water.db" \
    # - --ngpus=4 --lr=0.0005 --enable-hami=True --hami-weight=1  --batch-size=32 --lr-warmup-steps=1000 \
    # - --lr-schedule="polynomiallr" --max-steps=200000 --num-epochs=200000 --used-cache=True --train-ratio=500 --val-ratio=500 \
    # - --gradient-clip-val=5.0 --dataset-size=4999 --precision=64
    submit_args:
      env:
        {MKL_THREADING_LAYER: GNU}
      container_args:
        shm_size: 512g


